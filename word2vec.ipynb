{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['西南石油大学',\n",
      " '是',\n",
      " '新中国',\n",
      " '成立',\n",
      " '后',\n",
      " '的',\n",
      " '第二所',\n",
      " '是',\n",
      " '由',\n",
      " '本科',\n",
      " '院校',\n",
      " '。',\n",
      " '目前',\n",
      " '有',\n",
      " '成都',\n",
      " '和',\n",
      " '南充',\n",
      " '两个',\n",
      " '校区',\n",
      " '。',\n",
      " '成都',\n",
      " '校区',\n",
      " '有',\n",
      " '教职工',\n",
      " '2500',\n",
      " '多人',\n",
      " '，',\n",
      " '在校学生',\n",
      " '30000',\n",
      " '人',\n",
      " '。']\n",
      "['校区',\n",
      " '成都',\n",
      " '西南石油大学',\n",
      " '新中国',\n",
      " '2500',\n",
      " '30000',\n",
      " '第二所',\n",
      " '教职工',\n",
      " '在校学生',\n",
      " '南充',\n",
      " '本科',\n",
      " '院校',\n",
      " '多人',\n",
      " '成立',\n",
      " '两个',\n",
      " '目前']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "sentence =\"西南石油大学是新中国成立后的第二所是由本科院校。目前有成都和南充两个校区。成都校区有教职工2500多人，在校学生30000人。\"\n",
    "seg_list = jieba.cut(sentence)\n",
    "import pprint as pp\n",
    "jieba.add_word(\"新中国\")\n",
    "jieba.add_word(\"中国石油大学\")\n",
    "jieba.add_word(\"西南石油大学\")\n",
    "pp.pprint(list(seg_list))\n",
    "keywords = jieba.analyse.extract_tags(sentence)\n",
    "pp.pprint(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define get contents from files\n",
    "def get_content(path):\n",
    "    content =''\n",
    "    with open(path,'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for l in f:\n",
    "            content+=l\n",
    "    return content    \n",
    "content=get_content('D:\\\\NLP\\\\trainswpu.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "def get_stopwords(path):\n",
    "    stop_words=[word.strip() for word in open(path,encoding='utf-8').readlines()]\n",
    "    return stop_words\n",
    "stop_words = get_stopwords('D:\\\\NLP\\\\stopwords\\\\baidu.txt')\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r'\\，|\\（|\\）|\\、|\\“|\\”|\\。|')\n",
    "content = re.sub(pattern,'',content)\n",
    "# print(content)\n",
    "seg = jieba.cut(content)\n",
    "#glance at the segment result\n",
    "stop_words.append('！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unmeaningful word from seg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list =['的','用','中',' ','这','在','从','也','本','等','对','是','和','了','既','我们','！','\\ufeff','\\n','以','《','》']\n",
    "seg = [x for x in list(seg) if x not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "西南石油大学\n",
      "新中国\n",
      "创建\n",
      "第二所\n",
      "石油\n",
      "本科\n",
      "院校\n",
      "一所\n"
     ]
    }
   ],
   "source": [
    "# print(seg)\n",
    "m=1\n",
    "for i in seg:\n",
    "    if m>=9:\n",
    "        break\n",
    "    print(i)\n",
    "    m+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\NLP\\\\train.txt','w',encoding='utf-8') as f:\n",
    "    for l in seg:\n",
    "        f.write(l+\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 20:37:16,742 : INFO : collecting all words and their counts\n",
      "2019-05-11 20:37:16,743 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-11 20:37:16,744 : INFO : collected 1048 word types from a corpus of 3068 raw words and 1 sentences\n",
      "2019-05-11 20:37:16,745 : INFO : Loading a fresh vocabulary\n",
      "2019-05-11 20:37:16,747 : INFO : min_count=1 retains 1048 unique words (100% of original 1048, drops 0)\n",
      "2019-05-11 20:37:16,748 : INFO : min_count=1 leaves 3068 word corpus (100% of original 3068, drops 0)\n",
      "2019-05-11 20:37:16,752 : INFO : deleting the raw counts dictionary of 1048 items\n",
      "2019-05-11 20:37:16,752 : INFO : sample=0.001 downsamples 61 most-common words\n",
      "2019-05-11 20:37:16,753 : INFO : downsampling leaves estimated 2451 word corpus (79.9% of prior 3068)\n",
      "2019-05-11 20:37:16,755 : INFO : estimated required memory for 1048 words and 192 dimensions: 2133728 bytes\n",
      "2019-05-11 20:37:16,756 : INFO : resetting layer weights\n",
      "2019-05-11 20:37:16,783 : INFO : training model with 2 workers on 1048 vocabulary and 192 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-11 20:37:16,787 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 20:37:16,791 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 20:37:16,792 : INFO : EPOCH - 1 : training on 3068 raw words (2426 effective words) took 0.0s, 335613 effective words/s\n",
      "2019-05-11 20:37:16,797 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 20:37:16,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 20:37:16,802 : INFO : EPOCH - 2 : training on 3068 raw words (2444 effective words) took 0.0s, 348382 effective words/s\n",
      "2019-05-11 20:37:16,806 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 20:37:16,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 20:37:16,811 : INFO : EPOCH - 3 : training on 3068 raw words (2415 effective words) took 0.0s, 346419 effective words/s\n",
      "2019-05-11 20:37:16,816 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 20:37:16,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 20:37:16,822 : INFO : EPOCH - 4 : training on 3068 raw words (2440 effective words) took 0.0s, 279200 effective words/s\n",
      "2019-05-11 20:37:16,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 20:37:16,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 20:37:16,831 : INFO : EPOCH - 5 : training on 3068 raw words (2456 effective words) took 0.0s, 369807 effective words/s\n",
      "2019-05-11 20:37:16,832 : INFO : training on a 15340 raw words (12181 effective words) took 0.0s, 250232 effective words/s\n",
      "2019-05-11 20:37:16,832 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-11 20:37:16,833 : INFO : saving Word2Vec object under D:\\NLP\\swpu.word2vec, separately None\n",
      "2019-05-11 20:37:16,833 : INFO : not storing attribute vectors_norm\n",
      "2019-05-11 20:37:16,834 : INFO : not storing attribute cum_table\n",
      "2019-05-11 20:37:16,835 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-11 20:37:16,857 : INFO : saved D:\\NLP\\swpu.word2vec\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "corpus = open(\"D:\\\\NLP\\\\train.txt\",'r',encoding='utf-8')\n",
    "model = Word2Vec(LineSentence(corpus),sg=0,size=192,window=5,min_count=1,workers=2)\n",
    "model.save(\"D:\\\\NLP\\\\swpu.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似度： [('合作', 0.2574981153011322), ('共建', 0.24893036484718323), ('地质', 0.24363064765930176), ('四川', 0.24206829071044922), ('国', 0.2211296558380127), ('后劲', 0.2153310626745224), ('有', 0.21011190116405487), ('组建', 0.20978960394859314), ('十二五', 0.20667262375354767), ('1958', 0.2051476240158081), ('奖', 0.1970190852880478), ('留学生', 0.19564902782440186), ('全国', 0.19157297909259796), ('合同', 0.18884804844856262), ('五年制', 0.1860712766647339), ('石油化工', 0.18501919507980347), ('创业', 0.18228445947170258), ('带头人', 0.1773003786802292), ('真实', 0.17597126960754395), ('同意', 0.17304423451423645)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sim1 = model.most_similar(u'学术',topn=20)\n",
    "print('相似度：',sim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '人民', '名义', '', '', '周梅森', '', '', '', '', '©', '中文', '在线', '数字', '出版', '集团股份', '有限公司', '2016', '-', '2017', '', '', '数字', '版图', '书', '版权', '信息', '', '', '', '人民', '名义', '周梅森', '著', '.', '北京', '：', '中文', '在线', '数字', '出版', '集团股份', '有限公司', '2017.2', '.', '', '', 'CAEBN', '：', '7', '-', '001', '-', '000', '-', '60733638', '-', '6', '', '', '分类号', '：', '长篇小说', '', '—', '—', '', '中国', '', '—', '—', '', '当代', 'I247.54', '', '', '互联网', '出版', '许可证', '：', '新出', '网证京字', '045', '号', '', '', '', '人民', '名义', '', '', '周梅森', '著', '', '', '', '出', '', '品', '', '人', '：', '童之磊', '', '', '责任编辑', '：', '朱厚权', '', '', '出版发行', '：', '中文', '在线', '数字', '出版', '集团股份', '有限公司', '', '', '地址', '：', '北京市', '东城区', '安定门', '东大街', '28', '号', 'E座', '9', '层', '', '', '邮政编码', '：', '100007', '', '', '网址', '：', 'www', '.', 'chineseall', '.', '', '', '首次', '发布', '：', '2017.2', '.', '13', '', '', '更新', '时间', '：', '2017.3', '.', '1', '', '', '上架', '建议', '：', '小说', '', '', '本书', '北京', '十月', '文化传媒', '有限公司', '授权', '中文', '在线', '数字', '出版', '集团股份', '有限公司', '互联网', '出版', '发行', '未经', '书面', '授权', '地区', '方式', '反编译', '仿制', '节录', '本书', '文字', '图表', '本书', '电子版', '错讹', '敬请', '读者', '指正', '会', '更新', '版本', '', '', '电子邮箱', '：', 'copyright', '@', 'chineseall', '.', '', '', '中文', '在线', '数字', '出版', '集团股份', '有限公司', '作者', '相关', '机构', '提供', '数字', '出版', '服务', '', '', '纸质', '版图', '书', '版', '编目', '数据', '', '', '出版社', '：', '北京十月文艺出版社', '', '', 'ISBN', '：', '978', '-', '7', '-', '5302', '-', '1619', '-', '4', '', '', '出版', '时间', '：', '2017.1', '.', '1', '', '', '', '目录', '', '', '', '', '二', '', '', '三', '', '', '四', '', '', '五', '', '', '六', '', '', '七', '', '', '八', '', '', '九', '', '', '十', '', '', '十一', '', '', '十二', '', '', '十三', '', '', '十四', '', '', '十五', '', '', '十六', '', '', '十七', '', '', '十八', '', '', '十九', '', '', '二十', '', '', '二十一', '', '', '二十二', '', '', '二十三', '', '', '二十四', '', '', '二十五', '', '', '二十六', '', '', '二十七', '', '', '二十八', '', '', '二十九', '', '', '三十', '', '', '三十一', '', '', '三十二', '', '', '三十三', '', '', '三十四', '', '', '三十五', '', '', '三十六', '', '', '三十七', '', '', '三十八', '', '', '三十九', '', '', '四十', '', '', '四十一', '', '', '四十二', '', '', '四十三', '', '', '四十四', '', '', '四十五', '', '', '四十六', '', '', '四十七', '', '', '四十八', '', '', '四十九', '', '', '五十', '', '', '五十一', '', '', '五十二', '', '', '五十三', '', '', '五十四', '', '', '', '', '', '侯亮平', '得知', '航班', '无限期', '延误', '急得', '差点', '跳', '他本', '打算', '坐', '一班', '飞机', '赶往', 'H', '省', '协调', '指挥', '抓捕', '京州市', '副市长', '丁义珍', '这下子', '计划', '全', '落空', '广播', '中', '一遍', '遍', '传来', '女', '播音员', '中英文', '抱歉', '通知', '机场', '上空', '雷暴', '区', '乘客', '飞机', '暂时', '起飞', '侯亮平', '额上', '沁出', '一层', '细细的', '汗珠', '早', '被困', '机场', '痛苦', '尝', '滋味', '', '', '电视', '大', '荧屏', '正', '放映', '气象图', '团团', '浓厚', '白云', '呈', '旋涡', '状', '翻卷', '凶险', '样子', '字幕', '普及', '航空', '知识', '—', '—', '雷暴', '危及', '飞行', '误入', '雷暴', '区', '曾', '导致', '空难', '平息', '焦虑', '心情', '候机', '大厅', '变', '作', '巨型', '蜂巢', '嗡嗡嘤嘤', '噪声', '四起', '旅客', '分堆', '围住', '值机', '台', '机场', '工作人员', '吵吵嚷嚷', '无非', '打听', '航班', '起飞时间', '追问', '补偿', '方案', '侯亮平', '用不着', '往前', '凑', '明白', '一个', '：', '那片', '雷暴', '区', '头顶', '罩', '航班', '甭想', '上天', '', '', '侯亮平', '快步', '走出', '候机', '大厅', '寻', '僻静处', '一个', '接', '一个', '拨打', '手机号码', 'H', '省', '检察院', '检察长', '季昌明', '关机', '反贪局', '局长', '陈', '海关', '机当紧', '当忙', '全', '妈', '失踪', '侯亮平', '失踪', '参加', '一个', '紧急会议', '该省', '分管', '政法', '工作', '省委', '副', '书记', '高育良', '汇报', '丁义珍', '案件', '与会者', '都', '关机', '侯亮平', '存心', '关机', '玩', '失踪', '最高人民检察院', '反贪', '总局', '侦查', '处处长', '侯亮平', '反复', 'H', '省', '同行', '请求', '—', '—', '先抓', '人后', '开会', '姓', '丁', '副市长', '太', '刚', '侦破', '赵德汉', '受贿案', '关键', '一环', '走漏风声', '跑', 'H', '省', '官场', '上', '秘密', '石沉', '海底', '侯亮平', '大学', '同学', '陈海', '不满', '特地', '嘱咐', '陈海别', '汇报', '先', '丁义珍', '控制', '陈', '海胆', '小', '支吾', '几句', '到底', '汇报', '侯亮平', '正', '害怕', '夜长梦多', '抓捕', '赵德汉', '才', '第一', '时间', '夜间', '航班', '飞赴', 'H', '省', '不料', '偏', '陷入', '雷暴', '区', '', '', '侯亮平', '忽然', '发现', '外面', '无', '风', '无雨', '太平', '寂静', '穿梭', '送客', '喧闹', '车声', '消失', '雷暴', '？', '哪来', '雷暴', '区', '？', '跑', '出', '候机', '大厅', '门', '仰望', '夜空', '空中', '阴云密布', '月', '暗星', '晦', '看不见', '闪电', '更', '听', '不到', '雷声', '飞机', '起飞', '成', '一个', '谬误', '身边', '恰巧', '机场', '工作人员', '走过', '侯亮平', '拦住', '提出', '心中', '疑问', '这位', '上', '年纪', '老同志', '意味深长', '瞅', '一眼', '颇具', '哲理', '说', '看', '事物', '只', '看', '表面', '云层', '世界', '？', '平静', '藏', '雷暴', '侯亮平', '老同志', '背影', '发怔', '仿佛', '听到', '某种', '隐喻', '一番话', '使', '浮想联翩', '…', '…', '', '', '侯亮平', '毕业', 'H', '大学', '政法', '系', '老师', '同学', '遍布', 'H', '省', '官场', 'H', '省有', '一份', '格外', '牵挂', '反腐', '风暴', '愈演愈烈', 'H', '省', '平静', '异常', '这些年来', '此起彼伏', '传说', '大都', '止', '传说', '明白', '这是', '假象', '肉眼', '看不见', '云层', '世界', '看不见', '阳光', '下', '隐藏', '黑暗', '丁义珍', '浮出', '水面', '出于', '偶然', '赵德汉', '惊天大', '案', '牵扯', '一时半会儿', '还', '难以', '过硬', '证据', '侦查', '处处长', '深知', '时机', '重要性', '临门一脚', '胜负', '关键', '侯亮平', '着急', '再', '急', '没用', '天上', '雷暴', '挡', '', '', '安检', '回到', '候机', '大厅', '大厅', '里', '仍', '嘈杂', '强迫', '镇静', '饮水机', '前', '喝', '几口', '水找', '一处', '空', '椅子', '坐下', '闭目养神', '落网', '赵德汉', '形象', '适时', '浮现', '眼前', '禁不住', '沉浸', '赵德汉', '回忆', '中', '昨天晚上', '此人', '捧', '大', '海碗', '吃', '炸酱面', '时老旧', '木门', '吱呀', '一声', '开', '代表', '命运', '敲', '这位', '贪官', '家门', '', '', '贪官', '一脸', '憨厚', '相', '乍', '看上去', '不太像', '机关干部', '倒像', '刚', '下田', '回家', '老', '农民', '这位', '农民', '沉着', '冷静', '心理素质', '好', '处变不惊', '侯亮平', '一眼', '看透', '—', '—', '这是', '长期以来', '大权在握', '造就', '强势', '状态', '也许', '场面', '早', '预想', '中', '心理准备', '侯亮平', '没', '料到', '一个', '实名', '举报', '受贿', '几千万元', '部委', '项目', '处长', '竟然', '会住', '鬼', '地方', '', '', '这是', '一套', '常见', '机关', '房改房', '七十', '平方米', '老旧', '不堪', '家具', '像是', '赵德汉', '结婚', '时', '置办', '土得', '掉', '渣', '沙发', '边角', '都', '磨破', '门口', '丢', '几双', '破', '拖鞋', '扔', '街上', '都', '没人拾', '卫生间', '马桶', '漏水', '隔上', '三', '两秒钟', '滴答', '一声', '厨房', '里']\n"
     ]
    }
   ],
   "source": [
    "## 训练人民的名义\n",
    "jieba.suggest_freq('沙瑞金',True)\n",
    "jieba.suggest_freq('田国富',True)\n",
    "jieba.suggest_freq('高育良',True)\n",
    "jieba.suggest_freq('侯亮平',True)\n",
    "jieba.suggest_freq('钟小艾', True)\n",
    "jieba.suggest_freq('陈岩石', True)\n",
    "jieba.suggest_freq('欧阳菁', True)\n",
    "jieba.suggest_freq('易学习', True)\n",
    "jieba.suggest_freq('王大路', True)\n",
    "jieba.suggest_freq('蔡成功', True)\n",
    "jieba.suggest_freq('孙连城', True)\n",
    "jieba.suggest_freq('季昌明', True)\n",
    "jieba.suggest_freq('丁义珍', True)\n",
    "jieba.suggest_freq('郑西坡', True)\n",
    "jieba.suggest_freq('赵东来', True)\n",
    "jieba.suggest_freq('高小琴', True)\n",
    "jieba.suggest_freq('赵瑞龙', True)\n",
    "jieba.suggest_freq('林华华', True)\n",
    "jieba.suggest_freq('陆亦可', True)\n",
    "jieba.suggest_freq('刘新建', True)\n",
    "jieba.suggest_freq('刘庆祝', True)\n",
    "jieba.suggest_freq('京州市', True)\n",
    "jieba.suggest_freq('副市长', True)\n",
    "\n",
    "people = get_content('D:/NLP/people.txt')\n",
    "pattern = re.compile(r'\\，|\\（|\\）|\\、|\\“|\\”|\\。|\\n|\\u3000|/|\\ufeff|\\\" \"|')\n",
    "people = re.sub(pattern,'',people)\n",
    "seg_peo = list(jieba.cut(people,cut_all=False))\n",
    "seg_peo=[x.strip() for x in seg_peo if x not in stop_words]\n",
    "ceshi =['是有','北京','教育技术']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:/NLP/train_peo.txt\",'w',encoding='utf-8') as f:\n",
    "    for word in seg_peo:\n",
    "        f.write(word+' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 21:23:49,462 : INFO : collecting all words and their counts\n",
      "2019-05-11 21:23:49,463 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-11 21:23:49,465 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-11 21:23:49,509 : INFO : collected 17414 word types from a corpus of 88035 raw words and 9 sentences\n",
      "2019-05-11 21:23:49,510 : INFO : Loading a fresh vocabulary\n",
      "2019-05-11 21:23:49,538 : INFO : min_count=1 retains 17414 unique words (100% of original 17414, drops 0)\n",
      "2019-05-11 21:23:49,539 : INFO : min_count=1 leaves 88035 word corpus (100% of original 88035, drops 0)\n",
      "2019-05-11 21:23:49,593 : INFO : deleting the raw counts dictionary of 17414 items\n",
      "2019-05-11 21:23:49,594 : INFO : sample=0.001 downsamples 34 most-common words\n",
      "2019-05-11 21:23:49,594 : INFO : downsampling leaves estimated 78433 word corpus (89.1% of prior 88035)\n",
      "2019-05-11 21:23:49,639 : INFO : estimated required memory for 17414 words and 100 dimensions: 22638200 bytes\n",
      "2019-05-11 21:23:49,640 : INFO : resetting layer weights\n",
      "2019-05-11 21:23:49,883 : INFO : training model with 4 workers on 17414 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2019-05-11 21:23:49,885 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-11 21:23:49,958 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 21:23:49,963 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 21:23:49,971 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 21:23:49,972 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 21:23:49,972 : INFO : EPOCH - 1 : training on 88035 raw words (78329 effective words) took 0.1s, 934294 effective words/s\n",
      "2019-05-11 21:23:49,975 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-11 21:23:50,043 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 21:23:50,047 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 21:23:50,052 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 21:23:50,058 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 21:23:50,059 : INFO : EPOCH - 2 : training on 88035 raw words (78456 effective words) took 0.1s, 964065 effective words/s\n",
      "2019-05-11 21:23:50,060 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-11 21:23:50,131 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 21:23:50,135 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 21:23:50,144 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 21:23:50,145 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 21:23:50,146 : INFO : EPOCH - 3 : training on 88035 raw words (78491 effective words) took 0.1s, 946749 effective words/s\n",
      "2019-05-11 21:23:50,148 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-11 21:23:50,218 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 21:23:50,223 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 21:23:50,231 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 21:23:50,233 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 21:23:50,233 : INFO : EPOCH - 4 : training on 88035 raw words (78360 effective words) took 0.1s, 936736 effective words/s\n",
      "2019-05-11 21:23:50,236 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-11 21:23:50,305 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 21:23:50,310 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 21:23:50,318 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 21:23:50,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 21:23:50,322 : INFO : EPOCH - 5 : training on 88035 raw words (78381 effective words) took 0.1s, 928845 effective words/s\n",
      "2019-05-11 21:23:50,323 : INFO : training on a 440175 raw words (392017 effective words) took 0.4s, 892910 effective words/s\n",
      "2019-05-11 21:23:50,330 : INFO : saving Word2Vec object under D:/NLP/peo.word2vec, separately None\n",
      "2019-05-11 21:23:50,332 : INFO : not storing attribute vectors_norm\n",
      "2019-05-11 21:23:50,333 : INFO : not storing attribute cum_table\n",
      "2019-05-11 21:23:50,333 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-11 21:23:50,497 : INFO : saved D:/NLP/peo.word2vec\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "corpus2 = word2vec.Text8Corpus(r'D:/NLP/train_peo.txt')\n",
    "model2 = word2vec.Word2Vec(corpus2,sg=0,size=100,min_count=1,window=2,workers=4)\n",
    "model2.save('D:/NLP/peo.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-05-11 21:23:53,741 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('侯亮平', 0.9999293088912964), ('上', 0.9999173879623413), ('不', 0.9999141693115234), ('高育良', 0.9999034404754639), ('都', 0.9998998641967773), ('祁同伟', 0.999896764755249), ('沙瑞金', 0.9998952150344849), ('还', 0.9998900890350342), ('季昌明', 0.9998884201049805), ('陆亦可', 0.9998875260353088)]\n"
     ]
    }
   ],
   "source": [
    "print(model2.most_similar(u'李达康',topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998277486678868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model2.similarity('李达康','易学习'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#总结：\n",
    "# 1、使用gensim中的word2vec训练语料库（语料库要求：分此后的文本）\n",
    "# 2、然后进行相似性检测"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
